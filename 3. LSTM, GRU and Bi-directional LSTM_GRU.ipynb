{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214a251c-3e9d-465b-a783-287dc43ef1a1",
   "metadata": {},
   "source": [
    "# LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc4b69-5244-4985-8291-b4d73731971f",
   "metadata": {},
   "source": [
    "Simple RNN's were certainly better than classical ML algorithms and gave state of the art results, but it failed to capture long term dependencies that is present in sentences . So in 1998-99 LSTM's were introduced to counter to these drawbacks.\n",
    "\n",
    "## In Depth Understanding\n",
    "\n",
    "Why LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns\n",
    "* https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/\n",
    "\n",
    "What are LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm\n",
    "* https://distill.pub/2019/memorization-in-rnns/\n",
    "* https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6509a48e-ae2e-45f9-b0b7-68d14b56b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.imports import *\n",
    "\n",
    "# Refer to 1. Recurrent Neural Network for explanation\n",
    "xtrain, xvalid, ytrain, yvalid = prepare_data()\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 1500\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c376589-1202-42a8-8b96-ac2096d5a903",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "While building our simple RNN models we talked about using word-embeddings , So what is word-embeddings and how do we get word-embeddings?\n",
    "Here is the answer :\n",
    "\n",
    "- https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation\n",
    "- https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "\n",
    "The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors, word2vec and fasttext. In this Notebook, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip or you can search for GloVe in datasets on Kaggle and add the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118e30c-08c1-4401-874a-9d4947f7e77d",
   "metadata": {},
   "source": [
    "Commenting to save time,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ddf6e6-3495-4084-a4e6-5fbd1351c6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # load the GloVe vectors in a dictionary:\n",
    "\n",
    "# embeddings_index = {}\n",
    "# f = open('data/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "# for line in tqdm(f):\n",
    "#     values = line.split(' ')\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray([float(val) for val in values[1:]])\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37908820-098b-4dd3-8eec-afd3b4e4228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create an embedding matrix for the words we have in the dataset\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in tqdm(word_index.items()):\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9adbd79d-602a-4ab1-a9ad-e9d9e9509840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1500, 300)         4521300   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               160400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,681,801\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 4,521,300\n",
      "_________________________________________________________________\n",
      "Wall time: 3.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# A simple LSTM with glove embeddings and one dense layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1623324-eb05-4c5c-a866-4da2f65af628",
   "metadata": {},
   "source": [
    "As a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors . Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable = False.\n",
    "Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19af0db-b6ae-4e3c-bb42-7525bfb7fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64)\n",
    "\n",
    "# scores = model.predict(xvalid_pad)\n",
    "# print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712fb1b-d50c-4eb0-9267-48e0de1045d6",
   "metadata": {},
   "source": [
    "# Gated recurrent units (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57140004-6446-4cf4-aea3-ad1cfe4cad80",
   "metadata": {},
   "source": [
    "Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner.\n",
    "\n",
    "## In Depth Explanation\n",
    "\n",
    "* https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru\n",
    "* https://www.geeksforgeeks.org/gated-recurrent-unit-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c3545b-6778-4930-97e7-53b0708421ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1500, 300)         4521300   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 1500, 300)        0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 300)               541800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,063,401\n",
      "Trainable params: 5,063,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 753 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7f986-1aaf-4905-b81c-b41649b6604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64)\n",
    "\n",
    "# scores = model.predict(xvalid_pad)\n",
    "# print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b541173-77e4-4b3f-8c0b-24d336f9769c",
   "metadata": {},
   "source": [
    "# Bi-Directional LSTM/GRU Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef5d6ca0-49d8-429b-8d55-544f379c155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1500, 300)         4521300   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 600)              1442400   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 601       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,964,301\n",
      "Trainable params: 1,443,001\n",
      "Non-trainable params: 4,521,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d4870-3a54-4d5a-9fc3-4541707546ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64)\n",
    "\n",
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
