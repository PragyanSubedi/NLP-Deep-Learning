{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a43211b-7027-49b1-a449-1dfc7728112e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926e049-456e-4163-905e-fdbae544c9b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "* http://jalammar.github.io/illustrated-bert/ ---> In Depth Understanding of BERT\n",
    "\n",
    "After going through the post Above , I guess you must have understood how transformer architecture have been utilized by the current SOTA models . Now these architectures can be used in two ways :<br><br>\n",
    "1) We can use the model for prediction on our problems using the pretrained weights without fine-tuning or training the model for our sepcific tasks\n",
    "* EG: http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ ---> Using Pre-trained BERT without Tuning\n",
    "\n",
    "2) We can fine-tune or train these transformer models for our task by tweaking the already pre-trained weights and training on a much smaller dataset\n",
    "* EG:* https://www.youtube.com/watch?v=hinZO--TEk4&t=2933s ---> Tuning BERT For your TASK\n",
    "\n",
    "We will be using the first example as a base for our implementation of BERT model using Hugging Face and KERAS , but contrary to first example we will also Fine-Tune our model for our task\n",
    "\n",
    "Acknowledgements : https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc6027e-9b2a-4884-8dae-eaf54f7ac2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.imports import * \n",
    "\n",
    "# Loading Dependencies\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import transformers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28e8011-026b-4ed5-8ad2-f27ba21638d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATA\n",
    "\n",
    "train1 = pd.read_csv(\"./data/jigsaw-toxic-comment-train.csv\")\n",
    "valid = pd.read_csv('./data/validation.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51453bb9-31e3-4827-9457-ce26937279a5",
   "metadata": {},
   "source": [
    "Encoder For DATA for understanding waht encode batch does read documentation of hugging face tokenizer :\n",
    "https://huggingface.co/transformers/main_classes/tokenizer.html here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43352f98-d396-462b-b815-31447ea42fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbe7799-dbb2-4362-88e3-e857287ec804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMP DATA FOR CONFIG\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b01770-d955-46e5-a5f5-2b06a7288fb5",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8311b44-c9ac-45aa-af08-0e7062d4adf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "336ee22e-7dff-4193-a332-5a05b1e53735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 874/874 [00:24<00:00, 35.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 43.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [00:06<00:00, 38.12it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train1.toxic.values\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f00372-f3c8-4643-9dc0-24a41df47f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54dbf4fa-5a02-4109-9f8c-ec3f75b01d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    function for training the BERT model\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88c5bd-d698-4bd4-b112-ae2c27ad68d7",
   "metadata": {},
   "source": [
    "# Uncomment below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c6e3d9f-4d73-40b1-8fd2-84c73e7de144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# transformer_layer = (\n",
    "#     transformers.TFDistilBertModel\n",
    "#     .from_pretrained('distilbert-base-multilingual-cased')\n",
    "# )\n",
    "# model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52b4ce48-a6dd-49f8-9538-e3e553217bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "# train_history = model.fit(\n",
    "#     train_dataset,\n",
    "#     steps_per_epoch=n_steps,\n",
    "#     validation_data=valid_dataset,\n",
    "#     epochs=EPOCHS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7075f197-3b46-4188-8874-1dea6f262296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "# train_history_2 = model.fit(\n",
    "#     valid_dataset.repeat(),\n",
    "#     steps_per_epoch=n_steps,\n",
    "#     epochs=EPOCHS*2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf52d86-0113-41c8-87d4-71f62e373684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(test_dataset, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
