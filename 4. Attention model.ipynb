{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c97898-2d4e-41ae-9648-e96b980b0ecc",
   "metadata": {},
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a38e8a-e504-44fc-8001-f5701bae6692",
   "metadata": {},
   "source": [
    "This is the toughest and most tricky part. If you are able to understand the intiuition and working of attention block , understanding transformers and transformer based architectures like BERT will be a piece of cake. This is the part where I spent the most time on and I suggest you do the same . Please read and view the following resources in the order I am providing to ignore getting confused, also at the end of this try to write and draw an attention block in your own way :-\n",
    "\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/RDXpX/attention-model-intuition --> Only watch this video and not the next one\n",
    "* https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a\n",
    "* https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc\n",
    "* https://distill.pub/2016/augmented-rnns/ \n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "* https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ --> Basic Level\n",
    "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html ---> Implementation from Scratch in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d9975-2396-4eda-ba57-7b7729e78172",
   "metadata": {},
   "source": [
    "# Transformers : Attention is all you need\n",
    "\n",
    "So finally we have reached the end of the learning curve and are about to start learning the technology that changed NLP completely and are the reasons for the state of the art NLP techniques .Transformers were introduced in the paper Attention is all you need by Google. If you have understood the Attention models,this will be very easy , Here is transformers fully explained:\n",
    "\n",
    "* http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html ---> This presents the code implementation of the architecture presented in the paper by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f78532-892b-49c6-a858-7ec94951cb72",
   "metadata": {},
   "source": [
    "# BERT and Its Implementation on this Competition\n",
    "\n",
    "As Promised I am back with Resiurces , to understand about BERT architecture , please follow the contents in the given order :-\n",
    "\n",
    "* http://jalammar.github.io/illustrated-bert/ ---> In Depth Understanding of BERT\n",
    "\n",
    "After going through the post Above , I guess you must have understood how transformer architecture have been utilized by the current SOTA models . Now these architectures can be used in two ways :<br><br>\n",
    "1) We can use the model for prediction on our problems using the pretrained weights without fine-tuning or training the model for our sepcific tasks\n",
    "* EG: http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ ---> Using Pre-trained BERT without Tuning\n",
    "\n",
    "2) We can fine-tune or train these transformer models for our task by tweaking the already pre-trained weights and training on a much smaller dataset\n",
    "* EG:* https://www.youtube.com/watch?v=hinZO--TEk4&t=2933s ---> Tuning BERT For your TASK\n",
    "\n",
    "We will be using the first example as a base for our implementation of BERT model using Hugging Face and KERAS , but contrary to first example we will also Fine-Tune our model for our task\n",
    "\n",
    "Acknowledgements : https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "\n",
    "\n",
    "Steps Involved :\n",
    "* Data Preparation : Tokenization and encoding of data\n",
    "* Configuring TPU's \n",
    "* Building a Function for Model Training and adding an output layer for classification\n",
    "* Train the model and get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4087237-ef2e-4cb3-9731-d41e8c5e1bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
