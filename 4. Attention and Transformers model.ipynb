{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c97898-2d4e-41ae-9648-e96b980b0ecc",
   "metadata": {},
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a38e8a-e504-44fc-8001-f5701bae6692",
   "metadata": {},
   "source": [
    "This is the toughest and most tricky part. If you are able to understand the intiuition and working of attention block , understanding transformers and transformer based architectures like BERT will be a piece of cake. This is the part where I spent the most time on and I suggest you do the same . Please read and view the following resources in the order I am providing to ignore getting confused, also at the end of this try to write and draw an attention block in your own way :-\n",
    "\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/RDXpX/attention-model-intuition --> Only watch this video and not the next one\n",
    "* https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a\n",
    "* https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc\n",
    "* https://distill.pub/2016/augmented-rnns/ \n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "* https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/ --> Basic Level\n",
    "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html ---> Implementation from Scratch in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d9975-2396-4eda-ba57-7b7729e78172",
   "metadata": {},
   "source": [
    "# Transformers : Attention is all you need\n",
    "\n",
    "So finally we have reached the end of the learning curve and are about to start learning the technology that changed NLP completely and are the reasons for the state of the art NLP techniques .Transformers were introduced in the paper Attention is all you need by Google. If you have understood the Attention models,this will be very easy , Here is transformers fully explained:\n",
    "\n",
    "* http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html ---> This presents the code implementation of the architecture presented in the paper by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4087237-ef2e-4cb3-9731-d41e8c5e1bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
